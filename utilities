# Import the required libraries
import re
import os
import pandas as pd
import itertools
import numpy as np
from logger import logging
# from box.exceptions import BoxValueError
from box import ConfigBox
import yaml
import json
import joblib
from pathlib import Path
# from typing import Any
# from ensure import ensure_annotations
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, GridSearchCV
from exception import CustomException
import sys
import dill
from prophet.diagnostics import cross_validation
from prophet.plot import plot_cross_validation_metric

def read_yaml(path_to_yaml: Path) -> ConfigBox:
    try:
        with open(path_to_yaml) as yaml_file:
            content = yaml.safe_load(yaml_file)
            logging.info(f'yaml file: {path_to_yaml} loaded successfully')
            return ConfigBox(content)

    except BoxValueError:
        raise ValueError('yaml file is empty')
    except Exception as e:
        raise e

def save_json(path: Path, data: dict):
    """
    save json data

    Args:
    path (Path): path to json file
    data (dict): data to be saved in json file
    """
    with open(path, 'w') as f:
        json.dump(data, f, indent=4)

    logging.info(f'json file saved at: {path}')



def load_json(path: Path):
    """

    :param
        path (Path): path to json file

    :return:
        ConfigBox: data as class attributes instead of dict
    """
    with open(path) as f:
        content = json.load(f)

    logging.info(f'json file loaded successfully from: {path}')
    return content


def save_bin(data, path: Path):
    """
        save binary file

    :param
        data (Any): data to be saved as binary
        path (Path: path to binary file
    :return:
    """
    joblib.dump(value=data, filename=path)
    logging.info(f'binary file saved at: {path}')


def load_bin(path: Path):
    """
        load binary data
    :param
    path (Path): path to binary file

    :return:
        Any: object stored in the file
    """
    data = joblib.load(path)
    logging.info(f'binary file loaded from: {path}')
    return  data

def save_object(file_path: str | os.PathLike, obj):
    """
    Saves the object as a pickle file on the file_path provided
    
    """
    dir_path = os.path.dirname(file_path)
    os.makedirs(dir_path, exist_ok=True)
    with open(file_path, 'wb') as file_obj:
        dill.dump(obj, file_obj)


def load_object(file_path: str | os.PathLike):
    try:
        with open(file_path, "rb") as file_obj:
            return dill.load(file_obj)

    except Exception as e:
        raise CustomException(e, sys)
        
   
def evaluate_fbp_models(train, test, horizon, param):
    try:
        """
        This method fits and score the time series models provided while doing a gridsearch cross
        validation using the parameter grid provided
        
        input: train - Training time series data 
               test - Test time series data
               models - Time series ML model to experiment with
               param :dict - parameter settings to try as values.
             
        Returns: a dictionary of the a key values pair of model and score
        """ 
        def get_object(class_path):
            """
            get the class object from the string returned from YAML 
            """
            module_name, class_name = class_path.rsplit('.', 1)
            module = __import__(module_name, fromlist=[class_name])
            
            return getattr(module, class_name)()
        
        for model_name, class_path in models.items():
             models[model_name] = get_object(class_path)
                
        report = {}
     
        for i in range(len(list(models))):
            model = list(models.values())[i]
            para=param[list(models.keys())[i]]
                     
            gs = GridSearchCV(model, para, cv=3, verbose=3)
            gs.fit(train)

            model.set_params(**gs.best_params_)
            model.fit(train)

            forecast = 
            test_model_score = mean_squared_error(test, forecast)

            report[list(models.keys())[i]] = test_model_score
        return report
    except Exception as e:
        raise e
    



